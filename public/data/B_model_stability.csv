model,topics_used,min_n_per_topic,elasticity_var,stability,threshold_cut,percentile,method
Arcee AI Spotlight,8.0,12.0,0.13890943707114653,Stable,0.19669116707539924,0.7,OptionB_adaptive (slope|spearman)
Arcee Virtuoso Large,8.0,12.0,0.1654686717921677,Stable,0.19669116707539924,0.7,OptionB_adaptive (slope|spearman)
Baidu Ernie 4.5 21B A3B,8.0,12.0,0.18043197185587628,Stable,0.19669116707539924,0.7,OptionB_adaptive (slope|spearman)
Baidu: ERNIE 4.5 300B A47B,8.0,12.0,0.11137275530035802,Stable,0.19669116707539924,0.7,OptionB_adaptive (slope|spearman)
Claude 3.5 Haiku,8.0,12.0,0.20312604224765293,Chameleon,0.19669116707539924,0.7,OptionB_adaptive (slope|spearman)
Claude 3.5 Sonnet 20240620,8.0,12.0,0.16566098768157406,Stable,0.19669116707539924,0.7,OptionB_adaptive (slope|spearman)
Claude Sonnet 4,8.0,12.0,0.19844936253317974,Chameleon,0.19669116707539924,0.7,OptionB_adaptive (slope|spearman)
DeepSeek R1,8.0,12.0,0.18428681230470456,Stable,0.19669116707539924,0.7,OptionB_adaptive (slope|spearman)
DeepSeek V3,8.0,12.0,0.21230922588036152,Chameleon,0.19669116707539924,0.7,OptionB_adaptive (slope|spearman)
Gemini 2.0 flash,8.0,12.0,0.19535280035722866,Stable,0.19669116707539924,0.7,OptionB_adaptive (slope|spearman)
Gemini 2.5 Flash,8.0,12.0,0.24842672342229297,Chameleon,0.19669116707539924,0.7,OptionB_adaptive (slope|spearman)
Gemini 2.5 Flash Lite,8.0,12.0,0.17615957464170542,Stable,0.19669116707539924,0.7,OptionB_adaptive (slope|spearman)
Gemini 2.5 Pro,8.0,12.0,0.24171136572756963,Chameleon,0.19669116707539924,0.7,OptionB_adaptive (slope|spearman)
Gemma 3 12b,8.0,12.0,0.1574073365616176,Stable,0.19669116707539924,0.7,OptionB_adaptive (slope|spearman)
Gemma 3 27b,8.0,12.0,0.22033148159140492,Chameleon,0.19669116707539924,0.7,OptionB_adaptive (slope|spearman)
Gpt 3.5 Turbo,8.0,12.0,0.1361513363134138,Stable,0.19669116707539924,0.7,OptionB_adaptive (slope|spearman)
Gpt 4.1,8.0,12.0,0.1693815049837109,Stable,0.19669116707539924,0.7,OptionB_adaptive (slope|spearman)
Gpt 4.1 mini,8.0,12.0,0.16263857410465743,Stable,0.19669116707539924,0.7,OptionB_adaptive (slope|spearman)
Gpt 4o,8.0,12.0,0.17848499567163953,Stable,0.19669116707539924,0.7,OptionB_adaptive (slope|spearman)
Gpt 4o Mini,8.0,12.0,0.12402380334710628,Stable,0.19669116707539924,0.7,OptionB_adaptive (slope|spearman)
Gpt 5,8.0,12.0,0.23206779802173433,Chameleon,0.19669116707539924,0.7,OptionB_adaptive (slope|spearman)
Gpt 5 Nano,8.0,12.0,0.22114376630611823,Chameleon,0.19669116707539924,0.7,OptionB_adaptive (slope|spearman)
Gpt 5 mini,8.0,12.0,0.11064397835114843,Stable,0.19669116707539924,0.7,OptionB_adaptive (slope|spearman)
Grok 3 Mini,8.0,12.0,0.17826091293336238,Stable,0.19669116707539924,0.7,OptionB_adaptive (slope|spearman)
Grok 4,8.0,12.0,0.19649581202453476,Stable,0.19669116707539924,0.7,OptionB_adaptive (slope|spearman)
Hunyuan A13B Instruct,8.0,12.0,0.2279434147432664,Chameleon,0.19669116707539924,0.7,OptionB_adaptive (slope|spearman)
Inception Mercury,8.0,12.0,0.14193319279675837,Stable,0.19669116707539924,0.7,OptionB_adaptive (slope|spearman)
Kimi K2,8.0,12.0,0.18887517442033916,Stable,0.19669116707539924,0.7,OptionB_adaptive (slope|spearman)
Liquid LFM 40B MoE,8.0,12.0,0.12477895170634802,Stable,0.19669116707539924,0.7,OptionB_adaptive (slope|spearman)
Llama 3.1 8B Instruct,8.0,12.0,0.17449322092407046,Stable,0.19669116707539924,0.7,OptionB_adaptive (slope|spearman)
Llama 3.3 70b,8.0,12.0,0.139635837941203,Stable,0.19669116707539924,0.7,OptionB_adaptive (slope|spearman)
Llama 4 Maverick,8.0,12.0,0.17919620790750854,Stable,0.19669116707539924,0.7,OptionB_adaptive (slope|spearman)
Llama 4 Scout,8.0,12.0,0.18086441415010043,Stable,0.19669116707539924,0.7,OptionB_adaptive (slope|spearman)
MAI DS R1,8.0,12.0,0.14873639987412984,Stable,0.19669116707539924,0.7,OptionB_adaptive (slope|spearman)
Mistral Magistral Small 2506,8.0,12.0,0.22989169818898703,Chameleon,0.19669116707539924,0.7,OptionB_adaptive (slope|spearman)
Mistral Medium 3.1,8.0,12.0,0.15984313791159843,Stable,0.19669116707539924,0.7,OptionB_adaptive (slope|spearman)
Mistral Small 3.2 24b,8.0,12.0,0.16735895071261697,Stable,0.19669116707539924,0.7,OptionB_adaptive (slope|spearman)
Phi-4 reasoning+,8.0,12.0,0.15684251455762832,Stable,0.19669116707539924,0.7,OptionB_adaptive (slope|spearman)
Qwen 3 235b A22B Instruct,8.0,12.0,0.14337648215868845,Stable,0.19669116707539924,0.7,OptionB_adaptive (slope|spearman)
Qwen 3 32b,8.0,12.0,0.16986468726414947,Stable,0.19669116707539924,0.7,OptionB_adaptive (slope|spearman)
TDR Anubis 70b v1.1,8.0,12.0,0.2404299031487772,Chameleon,0.19669116707539924,0.7,OptionB_adaptive (slope|spearman)
TDR Anubis Pro 105b v1,8.0,12.0,0.16963695513445382,Stable,0.19669116707539924,0.7,OptionB_adaptive (slope|spearman)
Z-AI GLM 4.5,8.0,12.0,0.23268973685139074,Chameleon,0.19669116707539924,0.7,OptionB_adaptive (slope|spearman)
Z-AI GLM 4.5 Air,8.0,12.0,0.2130524739582468,Chameleon,0.19669116707539924,0.7,OptionB_adaptive (slope|spearman)
